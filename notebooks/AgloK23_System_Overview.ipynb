{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# AgloK23 Trading System - System Overview\n",
    "\n",
    "This notebook provides an overview of the AgloK23 quantitative trading system and demonstrates how to interact with its various components.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from datetime import datetime, timedelta\n",
    "\n",
    "# Add the src directory to Python path\n",
    "sys.path.append('../src')\n",
    "\n",
    "# Import AgloK23 modules\n",
    "from config.settings import Settings\n",
    "from core.data.models import MarketData, OHLCV\n",
    "from core.features.engine import FeatureEngine\n",
    "from core.models.manager import ModelManager\n",
    "\n",
    "plt.style.use('seaborn-v0_8')\n",
    "sns.set_palette(\"husl\")\n",
    "\n",
    "print(\"AgloK23 Trading System - Jupyter Environment Ready\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Configuration and Settings\n",
    "\n",
    "Let's start by loading the system configuration:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load system settings\n",
    "settings = Settings()\n",
    "\n",
    "print(\"System Configuration:\")\n",
    "print(f\"Environment: {settings.ENVIRONMENT}\")\n",
    "print(f\"Log Level: {settings.LOG_LEVEL}\")\n",
    "print(f\"Redis Host: {settings.REDIS_HOST}\")\n",
    "print(f\"Postgres Host: {settings.POSTGRES_HOST}\")\n",
    "print(f\"Paper Trading: {settings.ENABLE_PAPER_TRADING}\")\n",
    "print(f\"Default Portfolio Value: ${settings.DEFAULT_PORTFOLIO_VALUE:,.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Feature Engineering Demo\n",
    "\n",
    "Let's demonstrate the feature engineering capabilities:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create sample market data\n",
    "dates = pd.date_range(start='2024-01-01', end='2024-01-31', freq='H')\n",
    "np.random.seed(42)\n",
    "\n",
    "# Generate synthetic OHLCV data\n",
    "base_price = 50000\n",
    "returns = np.random.normal(0, 0.02, len(dates))\n",
    "prices = base_price * np.cumprod(1 + returns)\n",
    "\n",
    "sample_data = []\n",
    "for i, (date, price) in enumerate(zip(dates, prices)):\n",
    "    high = price * (1 + abs(np.random.normal(0, 0.005)))\n",
    "    low = price * (1 - abs(np.random.normal(0, 0.005)))\n",
    "    volume = np.random.uniform(100, 1000)\n",
    "    \n",
    "    ohlcv = OHLCV(\n",
    "        timestamp=date,\n",
    "        open=prices[i-1] if i > 0 else price,\n",
    "        high=high,\n",
    "        low=low,\n",
    "        close=price,\n",
    "        volume=volume\n",
    "    )\n",
    "    sample_data.append(ohlcv)\n",
    "\n",
    "print(f\"Generated {len(sample_data)} OHLCV data points\")\n",
    "\n",
    "# Convert to DataFrame for visualization\n",
    "df = pd.DataFrame([\n",
    "    {\n",
    "        'timestamp': d.timestamp,\n",
    "        'open': d.open,\n",
    "        'high': d.high,\n",
    "        'low': d.low,\n",
    "        'close': d.close,\n",
    "        'volume': d.volume\n",
    "    } for d in sample_data\n",
    "])\n",
    "\n",
    "df.set_index('timestamp', inplace=True)\n",
    "print(\"\\nSample data:\")\n",
    "print(df.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize the sample data\n",
    "fig, axes = plt.subplots(2, 1, figsize=(15, 10))\n",
    "\n",
    "# Price chart\n",
    "axes[0].plot(df.index, df['close'], label='Close Price', linewidth=1.5)\n",
    "axes[0].fill_between(df.index, df['low'], df['high'], alpha=0.3, label='High-Low Range')\n",
    "axes[0].set_title('BTC/USD Price Chart (Sample Data)')\n",
    "axes[0].set_ylabel('Price ($)')\n",
    "axes[0].legend()\n",
    "axes[0].grid(True, alpha=0.3)\n",
    "\n",
    "# Volume chart\n",
    "axes[1].bar(df.index, df['volume'], alpha=0.7, color='orange', width=0.02)\n",
    "axes[1].set_title('Trading Volume')\n",
    "axes[1].set_ylabel('Volume')\n",
    "axes[1].set_xlabel('Date')\n",
    "axes[1].grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Technical Indicators Analysis\n",
    "\n",
    "Let's compute some technical indicators:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate moving averages\n",
    "df['sma_20'] = df['close'].rolling(window=20).mean()\n",
    "df['sma_50'] = df['close'].rolling(window=50).mean()\n",
    "df['ema_20'] = df['close'].ewm(span=20).mean()\n",
    "\n",
    "# Calculate RSI\n",
    "def calculate_rsi(prices, window=14):\n",
    "    delta = prices.diff()\n",
    "    gain = (delta.where(delta > 0, 0)).rolling(window=window).mean()\n",
    "    loss = (-delta.where(delta < 0, 0)).rolling(window=window).mean()\n",
    "    rs = gain / loss\n",
    "    return 100 - (100 / (1 + rs))\n",
    "\n",
    "df['rsi'] = calculate_rsi(df['close'])\n",
    "\n",
    "# Calculate Bollinger Bands\n",
    "df['bb_middle'] = df['close'].rolling(window=20).mean()\n",
    "df['bb_std'] = df['close'].rolling(window=20).std()\n",
    "df['bb_upper'] = df['bb_middle'] + (df['bb_std'] * 2)\n",
    "df['bb_lower'] = df['bb_middle'] - (df['bb_std'] * 2)\n",
    "\n",
    "print(\"Technical indicators calculated:\")\n",
    "print(df[['close', 'sma_20', 'sma_50', 'ema_20', 'rsi']].tail())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize technical indicators\n",
    "fig, axes = plt.subplots(3, 1, figsize=(15, 12))\n",
    "\n",
    "# Price and moving averages\n",
    "axes[0].plot(df.index, df['close'], label='Close Price', linewidth=2)\n",
    "axes[0].plot(df.index, df['sma_20'], label='SMA 20', alpha=0.8)\n",
    "axes[0].plot(df.index, df['sma_50'], label='SMA 50', alpha=0.8)\n",
    "axes[0].plot(df.index, df['ema_20'], label='EMA 20', alpha=0.8)\n",
    "axes[0].fill_between(df.index, df['bb_lower'], df['bb_upper'], alpha=0.2, label='Bollinger Bands')\n",
    "axes[0].set_title('Price Chart with Moving Averages and Bollinger Bands')\n",
    "axes[0].set_ylabel('Price ($)')\n",
    "axes[0].legend()\n",
    "axes[0].grid(True, alpha=0.3)\n",
    "\n",
    "# RSI\n",
    "axes[1].plot(df.index, df['rsi'], label='RSI', color='purple', linewidth=2)\n",
    "axes[1].axhline(y=70, color='r', linestyle='--', alpha=0.7, label='Overbought (70)')\n",
    "axes[1].axhline(y=30, color='g', linestyle='--', alpha=0.7, label='Oversold (30)')\n",
    "axes[1].set_title('Relative Strength Index (RSI)')\n",
    "axes[1].set_ylabel('RSI')\n",
    "axes[1].set_ylim(0, 100)\n",
    "axes[1].legend()\n",
    "axes[1].grid(True, alpha=0.3)\n",
    "\n",
    "# Volume\n",
    "axes[2].bar(df.index, df['volume'], alpha=0.7, color='orange', width=0.02)\n",
    "axes[2].set_title('Trading Volume')\n",
    "axes[2].set_ylabel('Volume')\n",
    "axes[2].set_xlabel('Date')\n",
    "axes[2].grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Feature Correlation Analysis\n",
    "\n",
    "Let's analyze correlations between different features:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate returns and volatility\n",
    "df['returns'] = df['close'].pct_change()\n",
    "df['volatility'] = df['returns'].rolling(window=20).std()\n",
    "df['price_change'] = df['close'].diff()\n",
    "\n",
    "# Select features for correlation analysis\n",
    "feature_cols = ['returns', 'volatility', 'rsi', 'volume', 'price_change']\n",
    "correlation_data = df[feature_cols].dropna()\n",
    "\n",
    "# Calculate correlation matrix\n",
    "correlation_matrix = correlation_data.corr()\n",
    "\n",
    "# Plot correlation heatmap\n",
    "plt.figure(figsize=(10, 8))\n",
    "sns.heatmap(correlation_matrix, annot=True, cmap='coolwarm', center=0,\n",
    "            square=True, linewidths=0.5, fmt='.2f')\n",
    "plt.title('Feature Correlation Matrix')\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"\\nCorrelation Matrix:\")\n",
    "print(correlation_matrix.round(3))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Risk Metrics Analysis\n",
    "\n",
    "Let's calculate some basic risk metrics:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate risk metrics\n",
    "returns = df['returns'].dropna()\n",
    "\n",
    "# Basic statistics\n",
    "mean_return = returns.mean()\n",
    "std_return = returns.std()\n",
    "sharpe_ratio = mean_return / std_return * np.sqrt(24 * 365)  # Annualized for hourly data\n",
    "\n",
    "# Value at Risk (VaR) - 95% confidence\n",
    "var_95 = np.percentile(returns, 5)\n",
    "var_99 = np.percentile(returns, 1)\n",
    "\n",
    "# Maximum drawdown\n",
    "cumulative_returns = (1 + returns).cumprod()\n",
    "rolling_max = cumulative_returns.expanding().max()\n",
    "drawdown = (cumulative_returns - rolling_max) / rolling_max\n",
    "max_drawdown = drawdown.min()\n",
    "\n",
    "print(\"Risk Metrics:\")\n",
    "print(f\"Mean Return: {mean_return:.4f}\")\n",
    "print(f\"Return Volatility: {std_return:.4f}\")\n",
    "print(f\"Sharpe Ratio (Annualized): {sharpe_ratio:.2f}\")\n",
    "print(f\"VaR (95%): {var_95:.4f}\")\n",
    "print(f\"VaR (99%): {var_99:.4f}\")\n",
    "print(f\"Maximum Drawdown: {max_drawdown:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize risk metrics\n",
    "fig, axes = plt.subplots(2, 2, figsize=(15, 10))\n",
    "\n",
    "# Returns distribution\n",
    "axes[0, 0].hist(returns, bins=50, alpha=0.7, color='skyblue', edgecolor='black')\n",
    "axes[0, 0].axvline(var_95, color='red', linestyle='--', label=f'VaR 95%: {var_95:.4f}')\n",
    "axes[0, 0].axvline(var_99, color='darkred', linestyle='--', label=f'VaR 99%: {var_99:.4f}')\n",
    "axes[0, 0].set_title('Returns Distribution')\n",
    "axes[0, 0].set_xlabel('Returns')\n",
    "axes[0, 0].set_ylabel('Frequency')\n",
    "axes[0, 0].legend()\n",
    "axes[0, 0].grid(True, alpha=0.3)\n",
    "\n",
    "# Cumulative returns\n",
    "axes[0, 1].plot(cumulative_returns.index, cumulative_returns, label='Cumulative Returns')\n",
    "axes[0, 1].plot(rolling_max.index, rolling_max, label='Rolling Maximum', alpha=0.7)\n",
    "axes[0, 1].set_title('Cumulative Returns')\n",
    "axes[0, 1].set_ylabel('Cumulative Return')\n",
    "axes[0, 1].legend()\n",
    "axes[0, 1].grid(True, alpha=0.3)\n",
    "\n",
    "# Drawdown\n",
    "axes[1, 0].fill_between(drawdown.index, drawdown, 0, alpha=0.7, color='red')\n",
    "axes[1, 0].set_title('Drawdown')\n",
    "axes[1, 0].set_ylabel('Drawdown')\n",
    "axes[1, 0].set_xlabel('Date')\n",
    "axes[1, 0].grid(True, alpha=0.3)\n",
    "\n",
    "# Rolling volatility\n",
    "rolling_vol = returns.rolling(window=24).std()  # 24-hour rolling volatility\n",
    "axes[1, 1].plot(rolling_vol.index, rolling_vol, color='orange')\n",
    "axes[1, 1].set_title('Rolling Volatility (24h)')\n",
    "axes[1, 1].set_ylabel('Volatility')\n",
    "axes[1, 1].set_xlabel('Date')\n",
    "axes[1, 1].grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. System Health Check\n",
    "\n",
    "Let's check the status of various system components:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "import redis\n",
    "import psycopg2\n",
    "from datetime import datetime\n",
    "\n",
    "def check_service_health():\n",
    "    health_status = {}\n",
    "    \n",
    "    # Check Redis\n",
    "    try:\n",
    "        r = redis.Redis(host='localhost', port=6379, db=0)\n",
    "        r.ping()\n",
    "        health_status['Redis'] = 'âœ… Online'\n",
    "    except Exception as e:\n",
    "        health_status['Redis'] = f'âŒ Offline: {str(e)}'\n",
    "    \n",
    "    # Check PostgreSQL\n",
    "    try:\n",
    "        conn = psycopg2.connect(\n",
    "            host='localhost',\n",
    "            port=5432,\n",
    "            database='algok23',\n",
    "            user='postgres',\n",
    "            password='password'\n",
    "        )\n",
    "        conn.close()\n",
    "        health_status['PostgreSQL'] = 'âœ… Online'\n",
    "    except Exception as e:\n",
    "        health_status['PostgreSQL'] = f'âŒ Offline: {str(e)}'\n",
    "    \n",
    "    # Check TimescaleDB\n",
    "    try:\n",
    "        conn = psycopg2.connect(\n",
    "            host='localhost',\n",
    "            port=5433,\n",
    "            database='algok23_timeseries',\n",
    "            user='postgres',\n",
    "            password='password'\n",
    "        )\n",
    "        conn.close()\n",
    "        health_status['TimescaleDB'] = 'âœ… Online'\n",
    "    except Exception as e:\n",
    "        health_status['TimescaleDB'] = f'âŒ Offline: {str(e)}'\n",
    "    \n",
    "    # Check MLflow\n",
    "    try:\n",
    "        response = requests.get('http://localhost:5000/health', timeout=5)\n",
    "        if response.status_code == 200:\n",
    "            health_status['MLflow'] = 'âœ… Online'\n",
    "        else:\n",
    "            health_status['MLflow'] = f'âŒ HTTP {response.status_code}'\n",
    "    except Exception as e:\n",
    "        health_status['MLflow'] = f'âŒ Offline: {str(e)}'\n",
    "    \n",
    "    # Check Prometheus\n",
    "    try:\n",
    "        response = requests.get('http://localhost:9090/-/healthy', timeout=5)\n",
    "        if response.status_code == 200:\n",
    "            health_status['Prometheus'] = 'âœ… Online'\n",
    "        else:\n",
    "            health_status['Prometheus'] = f'âŒ HTTP {response.status_code}'\n",
    "    except Exception as e:\n",
    "        health_status['Prometheus'] = f'âŒ Offline: {str(e)}'\n",
    "    \n",
    "    # Check Grafana\n",
    "    try:\n",
    "        response = requests.get('http://localhost:3000/api/health', timeout=5)\n",
    "        if response.status_code == 200:\n",
    "            health_status['Grafana'] = 'âœ… Online'\n",
    "        else:\n",
    "            health_status['Grafana'] = f'âŒ HTTP {response.status_code}'\n",
    "    except Exception as e:\n",
    "        health_status['Grafana'] = f'âŒ Offline: {str(e)}'\n",
    "    \n",
    "    return health_status\n",
    "\n",
    "print(f\"System Health Check - {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "health = check_service_health()\n",
    "for service, status in health.items():\n",
    "    print(f\"{service:15} {status}\")\n",
    "\n",
    "print(\"=\" * 50)\n",
    "online_services = sum(1 for status in health.values() if 'âœ…' in status)\n",
    "total_services = len(health)\n",
    "print(f\"Services Online: {online_services}/{total_services}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Quick Start Commands\n",
    "\n",
    "Here are some useful commands to get started with the AgloK23 system:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"AgloK23 Quick Start Commands:\")\n",
    "print(\"\\nðŸ“Š System Monitoring:\")\n",
    "print(\"  - Grafana Dashboard: http://localhost:3000\")\n",
    "print(\"  - Prometheus Metrics: http://localhost:9090\")\n",
    "print(\"  - MLflow Tracking: http://localhost:5000\")\n",
    "\n",
    "print(\"\\nðŸ³ Docker Commands:\")\n",
    "print(\"  - Start services: docker-compose up -d\")\n",
    "print(\"  - Stop services: docker-compose down\")\n",
    "print(\"  - View logs: docker-compose logs -f\")\n",
    "print(\"  - Rebuild: docker-compose build --no-cache\")\n",
    "\n",
    "print(\"\\nðŸ”§ Development:\")\n",
    "print(\"  - Run tests: pytest tests/\")\n",
    "print(\"  - Format code: black src/\")\n",
    "print(\"  - Type check: mypy src/\")\n",
    "print(\"  - Start app: uvicorn src.main:app --reload\")\n",
    "\n",
    "print(\"\\nðŸ“ˆ Trading Operations:\")\n",
    "print(\"  - Check positions: curl http://localhost:8000/api/positions\")\n",
    "print(\"  - Health check: curl http://localhost:8000/health\")\n",
    "print(\"  - System metrics: curl http://localhost:8000/metrics\")\n",
    "\n",
    "print(\"\\nðŸ’¡ Next Steps:\")\n",
    "print(\"  1. Configure your .env file with API keys\")\n",
    "print(\"  2. Start the data ingestion services\")\n",
    "print(\"  3. Train your first ML models\")\n",
    "print(\"  4. Deploy trading strategies\")\n",
    "print(\"  5. Monitor performance via Grafana\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conclusion\n",
    "\n",
    "This notebook provides a basic overview of the AgloK23 trading system. The system includes:\n",
    "\n",
    "- **Data Pipeline**: Real-time and historical data ingestion from multiple sources\n",
    "- **Feature Engineering**: 100+ technical indicators and market features\n",
    "- **Machine Learning**: Multiple model types with ensemble capabilities\n",
    "- **Risk Management**: Portfolio risk metrics and position sizing\n",
    "- **Execution**: Smart order routing and multi-venue trading\n",
    "- **Monitoring**: Comprehensive system health and performance tracking\n",
    "\n",
    "To get started, make sure all services are running via Docker Compose and explore the various components through their respective interfaces."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
