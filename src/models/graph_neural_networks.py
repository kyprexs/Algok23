"""
Graph Neural Networks for Financial Markets
=========================================

Advanced GNN implementations for capturing cross-asset relationships,
supply chain dependencies, and market interconnections.

Features:
- Graph Attention Networks (GAT) for dynamic relationship weighting
- GraphSAGE for scalable node embedding learning
- Temporal Graph Networks for time-evolving relationships
- Financial graph construction from correlation and flow data
- Integration with trading signals and portfolio optimization
"""

import logging
import numpy as np
import pandas as pd
import torch
import torch.nn as nn
import torch.nn.functional as F
from torch_geometric.nn import GATConv, SAGEConv, GCNConv, global_mean_pool
from torch_geometric.data import Data, DataLoader
from torch_geometric.utils import to_networkx
import networkx as nx
from typing import Dict, List, Optional, Tuple, Any
from datetime import datetime, timedelta
import asyncio
from collections import defaultdict

logger = logging.getLogger(__name__)


class FinancialGraphConstructor:
    """
    Constructs financial graphs from market data and alternative sources.
    """
    
    def __init__(self, 
                 correlation_threshold: float = 0.3,
                 volatility_lookback: int = 30,
                 update_frequency: str = 'daily'):
        self.correlation_threshold = correlation_threshold
        self.volatility_lookback = volatility_lookback
        self.update_frequency = update_frequency
        
        # Graph data storage
        self.asset_nodes = {}
        self.correlation_edges = {}
        self.flow_edges = {}
        self.sector_edges = {}
        
        # Node features
        self.node_features = {}
        
        logger.info("FinancialGraphConstructor initialized")
    
    def build_correlation_graph(self, 
                              price_data: pd.DataFrame,
                              assets: List[str]) -> Data:
        """
        Build correlation graph from price time series.
        
        Args:
            price_data: DataFrame with asset prices
            assets: List of asset symbols
            
        Returns:
            PyTorch Geometric Data object
        """
        try:
            # Calculate correlation matrix
            returns = price_data[assets].pct_change().dropna()
            corr_matrix = returns.corr()
            
            # Create node features (asset characteristics)
            node_features = []
            node_mapping = {asset: idx for idx, asset in enumerate(assets)}
            
            for asset in assets:
                features = self._extract_asset_features(price_data, asset)
                node_features.append(features)
            
            # Create edges based on correlation threshold
            edge_indices = []
            edge_attributes = []
            
            for i, asset_i in enumerate(assets):
                for j, asset_j in enumerate(assets):
                    if i != j:
                        corr = corr_matrix.loc[asset_i, asset_j]
                        if abs(corr) > self.correlation_threshold:
                            edge_indices.append([i, j])
                            edge_attributes.append([
                                corr,  # Correlation
                                abs(corr),  # Absolute correlation
                                1.0 if corr > 0 else 0.0,  # Positive correlation flag
                                self._calculate_cointegration(returns[asset_i], returns[asset_j])
                            ])
            
            # Convert to tensors
            x = torch.tensor(node_features, dtype=torch.float)
            edge_index = torch.tensor(edge_indices, dtype=torch.long).t().contiguous()
            edge_attr = torch.tensor(edge_attributes, dtype=torch.float)
            
            # Create graph data object
            data = Data(x=x, edge_index=edge_index, edge_attr=edge_attr)
            
            # Add metadata
            data.asset_names = assets
            data.node_mapping = node_mapping
            data.graph_type = 'correlation'
            data.timestamp = datetime.now()
            
            logger.info(f"Built correlation graph: {len(assets)} nodes, {edge_index.size(1)} edges")
            return data
            
        except Exception as e:
            logger.error(f"Error building correlation graph: {e}")
            return None
    
    def _extract_asset_features(self, price_data: pd.DataFrame, asset: str) -> List[float]:
        """Extract node features for an asset."""
        try:
            prices = price_data[asset].dropna()
            returns = prices.pct_change().dropna()
            
            # Price-based features
            current_price = prices.iloc[-1]
            price_change_1d = prices.pct_change(1).iloc[-1]
            price_change_5d = prices.pct_change(5).iloc[-1]
            price_change_20d = prices.pct_change(20).iloc[-1]
            
            # Volatility features
            volatility_1d = returns.rolling(1).std().iloc[-1]
            volatility_5d = returns.rolling(5).std().iloc[-1]
            volatility_20d = returns.rolling(20).std().iloc[-1]
            
            # Technical features
            sma_20 = prices.rolling(20).mean().iloc[-1]
            price_to_sma = current_price / sma_20 if sma_20 > 0 else 1.0
            
            # Volume features (if available)
            volume_feature = 1.0  # Placeholder
            
            # Momentum features
            momentum_5d = (current_price / prices.shift(5).iloc[-1] - 1) if len(prices) > 5 else 0.0
            momentum_20d = (current_price / prices.shift(20).iloc[-1] - 1) if len(prices) > 20 else 0.0
            
            features = [
                np.log(current_price) if current_price > 0 else 0.0,  # Log price
                price_change_1d,  # 1-day return
                price_change_5d,  # 5-day return
                price_change_20d,  # 20-day return
                volatility_1d,  # 1-day volatility
                volatility_5d,  # 5-day volatility
                volatility_20d,  # 20-day volatility
                price_to_sma,  # Price relative to SMA
                volume_feature,  # Volume indicator
                momentum_5d,  # 5-day momentum
                momentum_20d,  # 20-day momentum
                len(prices)  # Data availability
            ]
            
            # Handle NaN values
            features = [0.0 if np.isnan(x) or np.isinf(x) else float(x) for x in features]
            
            return features
            
        except Exception as e:
            logger.error(f"Error extracting features for {asset}: {e}")
            return [0.0] * 12  # Return zero features as fallback
    
    def _calculate_cointegration(self, series1: pd.Series, series2: pd.Series) -> float:
        """
        Simple cointegration measure using correlation of differences.
        """
        try:
            if len(series1) != len(series2) or len(series1) < 2:
                return 0.0
            
            # Simple proxy: correlation of price changes
            diff1 = series1.diff().dropna()
            diff2 = series2.diff().dropna()
            
            if len(diff1) == len(diff2) and len(diff1) > 0:
                correlation = np.corrcoef(diff1, diff2)[0, 1]
                return correlation if not np.isnan(correlation) else 0.0
            
            return 0.0
            
        except Exception:
            return 0.0
    
    def build_supply_chain_graph(self, supply_chain_data: Dict[str, List[str]]) -> Data:
        """
        Build supply chain dependency graph.
        
        Args:
            supply_chain_data: Dict mapping companies to their suppliers/customers
            
        Returns:
            PyTorch Geometric Data object
        """
        try:
            # Get all unique companies
            all_companies = set()
            for company, connections in supply_chain_data.items():
                all_companies.add(company)
                all_companies.update(connections)
            
            all_companies = list(all_companies)
            company_to_idx = {company: idx for idx, company in enumerate(all_companies)}
            
            # Create node features (company characteristics)
            node_features = []
            for company in all_companies:
                # Placeholder features - would be filled from company data
                features = [
                    hash(company) % 100 / 100.0,  # Company identifier hash
                    len(supply_chain_data.get(company, [])),  # Number of connections
                    1.0  # Active flag
                ]
                node_features.append(features)
            
            # Create edges
            edge_indices = []
            edge_attributes = []
            
            for company, connections in supply_chain_data.items():
                company_idx = company_to_idx[company]
                for connection in connections:
                    if connection in company_to_idx:
                        connection_idx = company_to_idx[connection]
                        edge_indices.append([company_idx, connection_idx])
                        edge_attributes.append([1.0, 1.0])  # Supply chain strength, direction
            
            # Convert to tensors
            x = torch.tensor(node_features, dtype=torch.float)
            edge_index = torch.tensor(edge_indices, dtype=torch.long).t().contiguous()
            edge_attr = torch.tensor(edge_attributes, dtype=torch.float)
            
            data = Data(x=x, edge_index=edge_index, edge_attr=edge_attr)
            data.company_names = all_companies
            data.company_mapping = company_to_idx
            data.graph_type = 'supply_chain'
            
            logger.info(f"Built supply chain graph: {len(all_companies)} nodes, {edge_index.size(1)} edges")
            return data
            
        except Exception as e:
            logger.error(f"Error building supply chain graph: {e}")
            return None


class GraphAttentionNetwork(nn.Module):
    """
    Graph Attention Network for financial prediction.
    """
    
    def __init__(self,
                 num_features: int,
                 hidden_dim: int = 64,
                 num_heads: int = 8,
                 num_layers: int = 3,
                 dropout: float = 0.2,
                 output_dim: int = 3):
        super().__init__()
        
        self.num_features = num_features
        self.hidden_dim = hidden_dim
        self.num_heads = num_heads
        self.num_layers = num_layers
        self.dropout = dropout
        
        # Input projection
        self.input_projection = nn.Linear(num_features, hidden_dim)
        
        # GAT layers
        self.gat_layers = nn.ModuleList()
        for i in range(num_layers):
            in_dim = hidden_dim if i == 0 else hidden_dim * num_heads
            out_dim = hidden_dim
            
            self.gat_layers.append(
                GATConv(
                    in_channels=in_dim,
                    out_channels=out_dim,
                    heads=num_heads,
                    dropout=dropout,
                    concat=True if i < num_layers - 1 else False
                )
            )
        
        # Output layers
        final_dim = hidden_dim * num_heads if num_layers > 1 else hidden_dim
        self.output_layers = nn.Sequential(
            nn.Linear(final_dim, hidden_dim),
            nn.ReLU(),
            nn.Dropout(dropout),
            nn.Linear(hidden_dim, hidden_dim // 2),
            nn.ReLU(),
            nn.Dropout(dropout),
            nn.Linear(hidden_dim // 2, output_dim)
        )
        
        # Global pooling
        self.global_pool = global_mean_pool
        
    def forward(self, data):
        x, edge_index = data.x, data.edge_index
        batch = getattr(data, 'batch', None)
        
        # Input projection
        x = self.input_projection(x)
        
        # GAT layers
        for i, gat_layer in enumerate(self.gat_layers):
            x = gat_layer(x, edge_index)
            if i < len(self.gat_layers) - 1:  # Don't apply activation to last layer
                x = F.elu(x)
                x = F.dropout(x, p=self.dropout, training=self.training)
        
        # Global pooling for graph-level prediction
        if batch is not None:
            x = self.global_pool(x, batch)
        else:
            # Single graph case
            x = x.mean(dim=0, keepdim=True)
        
        # Output prediction
        out = self.output_layers(x)
        return F.softmax(out, dim=1)


class GraphSAGENetwork(nn.Module):
    """
    GraphSAGE network for scalable graph learning.
    """
    
    def __init__(self,
                 num_features: int,
                 hidden_dim: int = 64,
                 num_layers: int = 3,
                 dropout: float = 0.2,
                 output_dim: int = 3):
        super().__init__()
        
        self.num_layers = num_layers
        self.dropout = dropout
        
        # SAGE layers
        self.sage_layers = nn.ModuleList()
        
        # First layer
        self.sage_layers.append(SAGEConv(num_features, hidden_dim))
        
        # Hidden layers
        for _ in range(num_layers - 2):
            self.sage_layers.append(SAGEConv(hidden_dim, hidden_dim))
        
        # Last layer
        self.sage_layers.append(SAGEConv(hidden_dim, hidden_dim))
        
        # Output layers
        self.output_layers = nn.Sequential(
            nn.Linear(hidden_dim, hidden_dim // 2),
            nn.ReLU(),
            nn.Dropout(dropout),
            nn.Linear(hidden_dim // 2, output_dim)
        )
    
    def forward(self, data):
        x, edge_index = data.x, data.edge_index
        batch = getattr(data, 'batch', None)
        
        # SAGE layers
        for i, sage_layer in enumerate(self.sage_layers):
            x = sage_layer(x, edge_index)
            if i < len(self.sage_layers) - 1:
                x = F.relu(x)
                x = F.dropout(x, p=self.dropout, training=self.training)
        
        # Global pooling
        if batch is not None:
            x = global_mean_pool(x, batch)
        else:
            x = x.mean(dim=0, keepdim=True)
        
        # Output prediction
        out = self.output_layers(x)
        return F.softmax(out, dim=1)


class FinancialGNNPredictor:
    """
    Main predictor class combining graph construction and GNN models.
    """
    
    def __init__(self,
                 model_type: str = 'GAT',
                 hidden_dim: int = 64,
                 num_layers: int = 3,
                 learning_rate: float = 0.001):
        
        self.model_type = model_type
        self.hidden_dim = hidden_dim
        self.num_layers = num_layers
        self.learning_rate = learning_rate
        
        # Components
        self.graph_constructor = FinancialGraphConstructor()
        self.model = None
        self.optimizer = None
        self.device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
        
        # Training state
        self.is_trained = False
        self.training_history = []
        
        logger.info(f"FinancialGNNPredictor initialized with {model_type} on {self.device}")
    
    def build_model(self, num_features: int, output_dim: int = 3):
        """Build the GNN model."""
        if self.model_type == 'GAT':
            self.model = GraphAttentionNetwork(
                num_features=num_features,
                hidden_dim=self.hidden_dim,
                num_layers=self.num_layers,
                output_dim=output_dim
            )
        elif self.model_type == 'SAGE':
            self.model = GraphSAGENetwork(
                num_features=num_features,
                hidden_dim=self.hidden_dim,
                num_layers=self.num_layers,
                output_dim=output_dim
            )
        else:
            raise ValueError(f"Unsupported model type: {self.model_type}")
        
        self.model = self.model.to(self.device)
        self.optimizer = torch.optim.Adam(self.model.parameters(), lr=self.learning_rate)
        
        logger.info(f"Built {self.model_type} model with {sum(p.numel() for p in self.model.parameters())} parameters")
    
    def train_model(self,
                    train_graphs: List[Data],
                    train_labels: List[int],
                    val_graphs: List[Data] = None,
                    val_labels: List[int] = None,
                    epochs: int = 100):
        """
        Train the GNN model.
        
        Args:
            train_graphs: List of training graphs
            train_labels: Training labels
            val_graphs: Validation graphs
            val_labels: Validation labels
            epochs: Number of training epochs
        """
        if self.model is None:
            # Infer feature dimension from first graph
            num_features = train_graphs[0].x.size(1)
            self.build_model(num_features)
        
        self.model.train()
        criterion = nn.CrossEntropyLoss()
        
        # Create data loader
        train_data = [(graph, label) for graph, label in zip(train_graphs, train_labels)]
        
        best_val_accuracy = 0.0
        patience_counter = 0
        patience = 10
        
        for epoch in range(epochs):
            total_loss = 0.0
            correct = 0
            total = 0
            
            # Training loop
            for graph, label in train_data:
                self.optimizer.zero_grad()
                
                graph = graph.to(self.device)
                label_tensor = torch.tensor([label], dtype=torch.long).to(self.device)
                
                output = self.model(graph)
                loss = criterion(output, label_tensor)
                
                loss.backward()
                self.optimizer.step()
                
                total_loss += loss.item()
                predicted = output.argmax(dim=1)
                correct += (predicted == label_tensor).sum().item()
                total += 1
            
            train_accuracy = correct / total if total > 0 else 0.0
            avg_loss = total_loss / len(train_data) if train_data else 0.0
            
            # Validation
            val_accuracy = 0.0
            if val_graphs and val_labels:
                val_accuracy = self._evaluate(val_graphs, val_labels)
                
                # Early stopping
                if val_accuracy > best_val_accuracy:
                    best_val_accuracy = val_accuracy
                    patience_counter = 0
                else:
                    patience_counter += 1
                    if patience_counter >= patience:
                        logger.info(f"Early stopping at epoch {epoch}")
                        break
            
            # Log progress
            if epoch % 20 == 0:
                logger.info(f"Epoch {epoch}: Loss={avg_loss:.4f}, "
                          f"Train Acc={train_accuracy:.4f}, Val Acc={val_accuracy:.4f}")
            
            # Store history
            self.training_history.append({
                'epoch': epoch,
                'loss': avg_loss,
                'train_accuracy': train_accuracy,
                'val_accuracy': val_accuracy
            })
        
        self.is_trained = True
        logger.info(f"Training completed. Best validation accuracy: {best_val_accuracy:.4f}")
    
    def _evaluate(self, graphs: List[Data], labels: List[int]) -> float:
        """Evaluate model on given graphs and labels."""
        self.model.eval()
        correct = 0
        total = 0
        
        with torch.no_grad():
            for graph, label in zip(graphs, labels):
                graph = graph.to(self.device)
                output = self.model(graph)
                predicted = output.argmax(dim=1).item()
                correct += (predicted == label)
                total += 1
        
        self.model.train()
        return correct / total if total > 0 else 0.0
    
    def predict(self, graph: Data) -> Tuple[int, float]:
        """
        Make prediction on a single graph.
        
        Returns:
            Tuple of (predicted_class, confidence)
        """
        if not self.is_trained:
            logger.warning("Model not trained yet")
            return 0, 0.0
        
        self.model.eval()
        with torch.no_grad():
            graph = graph.to(self.device)
            output = self.model(graph)
            probabilities = F.softmax(output, dim=1)
            predicted_class = output.argmax(dim=1).item()
            confidence = probabilities[0, predicted_class].item()
        
        self.model.train()
        return predicted_class, confidence
    
    async def predict_market_direction(self,
                                     price_data: pd.DataFrame,
                                     assets: List[str]) -> Dict[str, Any]:
        """
        Predict market direction using graph relationships.
        
        Args:
            price_data: Historical price data
            assets: List of assets to analyze
            
        Returns:
            Dictionary with prediction results
        """
        try:
            # Build correlation graph
            graph = self.graph_constructor.build_correlation_graph(price_data, assets)
            if graph is None:
                return {'error': 'Failed to build graph'}
            
            # Make prediction
            prediction, confidence = self.predict(graph)
            
            # Interpret prediction
            direction_map = {0: 'bearish', 1: 'neutral', 2: 'bullish'}
            direction = direction_map.get(prediction, 'unknown')
            
            return {
                'prediction': prediction,
                'direction': direction,
                'confidence': confidence,
                'graph_nodes': len(assets),
                'graph_edges': graph.edge_index.size(1),
                'timestamp': datetime.now().isoformat()
            }
            
        except Exception as e:
            logger.error(f"Error in market direction prediction: {e}")
            return {'error': str(e)}


# Example usage and test functions
def create_synthetic_graph_data(num_graphs: int = 100,
                              num_nodes: int = 20,
                              num_features: int = 12) -> Tuple[List[Data], List[int]]:
    """Create synthetic graph data for testing."""
    graphs = []
    labels = []
    
    for _ in range(num_graphs):
        # Random node features
        x = torch.randn(num_nodes, num_features)
        
        # Random edges (ensuring some connectivity)
        num_edges = np.random.randint(num_nodes, num_nodes * 3)
        edge_indices = []
        for _ in range(num_edges):
            src = np.random.randint(0, num_nodes)
            dst = np.random.randint(0, num_nodes)
            if src != dst:
                edge_indices.append([src, dst])
        
        if edge_indices:
            edge_index = torch.tensor(edge_indices, dtype=torch.long).t().contiguous()
            edge_attr = torch.randn(len(edge_indices), 2)
        else:
            # Fallback: create self-loops
            edge_index = torch.tensor([[i, i] for i in range(num_nodes)], dtype=torch.long).t().contiguous()
            edge_attr = torch.ones(num_nodes, 2)
        
        # Create graph
        graph = Data(x=x, edge_index=edge_index, edge_attr=edge_attr)
        
        # Random label (0=bearish, 1=neutral, 2=bullish)
        label = np.random.randint(0, 3)
        
        graphs.append(graph)
        labels.append(label)
    
    return graphs, labels


async def test_gnn_system():
    """Test the complete GNN system."""
    logger.info("üß† Testing Financial GNN System...")
    
    try:
        # Create synthetic data
        train_graphs, train_labels = create_synthetic_graph_data(80, 15, 12)
        val_graphs, val_labels = create_synthetic_graph_data(20, 15, 12)
        
        logger.info(f"Created {len(train_graphs)} training graphs, {len(val_graphs)} validation graphs")
        
        # Test GAT model
        gnn_predictor = FinancialGNNPredictor(model_type='GAT', hidden_dim=32, num_layers=2)
        gnn_predictor.train_model(train_graphs, train_labels, val_graphs, val_labels, epochs=50)
        
        # Test prediction
        test_graph, _ = create_synthetic_graph_data(1, 15, 12)
        prediction, confidence = gnn_predictor.predict(test_graph[0])
        
        logger.info(f"‚úÖ GAT Model trained and tested")
        logger.info(f"   Sample prediction: class={prediction}, confidence={confidence:.4f}")
        
        # Test GraphSAGE model
        sage_predictor = FinancialGNNPredictor(model_type='SAGE', hidden_dim=32, num_layers=2)
        sage_predictor.train_model(train_graphs, train_labels, val_graphs, val_labels, epochs=50)
        
        prediction, confidence = sage_predictor.predict(test_graph[0])
        logger.info(f"‚úÖ GraphSAGE Model trained and tested")
        logger.info(f"   Sample prediction: class={prediction}, confidence={confidence:.4f}")
        
        # Test with synthetic price data
        dates = pd.date_range(start='2023-01-01', periods=100, freq='D')
        assets = ['AAPL', 'GOOGL', 'MSFT', 'TSLA', 'META']
        
        # Create synthetic price data
        price_data = {}
        for asset in assets:
            prices = 100 * np.exp(np.cumsum(np.random.normal(0.001, 0.02, len(dates))))
            price_data[asset] = prices
        
        price_df = pd.DataFrame(price_data, index=dates)
        
        # Test market direction prediction
        result = await gnn_predictor.predict_market_direction(price_df, assets)
        logger.info(f"‚úÖ Market direction prediction: {result}")
        
        return True
        
    except Exception as e:
        logger.error(f"‚ùå GNN system test failed: {e}")
        return False


if __name__ == "__main__":
    import asyncio
    asyncio.run(test_gnn_system())
